{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting OCR and Layout Analysis Pipeline...\n",
      "Processing: D:/Fraud Detection/data/SROIE Dataset/SROIE2019/SROIE2019/task1_train/X00016469612.jpg\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 297\u001b[0m\n\u001b[0;32m    294\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOCR and Layout Analysis Complete.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    296\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 297\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[2], line 293\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    291\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmain\u001b[39m():\n\u001b[0;32m    292\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting OCR and Layout Analysis Pipeline...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 293\u001b[0m     \u001b[43mprocess_sroie_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    294\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOCR and Layout Analysis Complete.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[2], line 255\u001b[0m, in \u001b[0;36mprocess_sroie_dataset\u001b[1;34m()\u001b[0m\n\u001b[0;32m    251\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessing: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimage_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    253\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    254\u001b[0m     \u001b[38;5;66;03m# Step 1: Perform OCR on the image\u001b[39;00m\n\u001b[1;32m--> 255\u001b[0m     boxes, text \u001b[38;5;241m=\u001b[39m \u001b[43mperform_ocr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    257\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m text:\n\u001b[0;32m    258\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo text detected in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimage_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, skipping...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[2], line 75\u001b[0m, in \u001b[0;36mperform_ocr\u001b[1;34m(image_path)\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mperform_ocr\u001b[39m(image_path):\n\u001b[0;32m     73\u001b[0m     \u001b[38;5;66;03m# Initialize EasyOCR reader\u001b[39;00m\n\u001b[0;32m     74\u001b[0m     reader \u001b[38;5;241m=\u001b[39m easyocr\u001b[38;5;241m.\u001b[39mReader([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124men\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m---> 75\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mreader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadtext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     77\u001b[0m     \u001b[38;5;66;03m# Extract bounding boxes and text\u001b[39;00m\n\u001b[0;32m     78\u001b[0m     boxes \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\easyocr\\easyocr.py:468\u001b[0m, in \u001b[0;36mReader.readtext\u001b[1;34m(self, image, decoder, beamWidth, batch_size, workers, allowlist, blocklist, detail, rotation_info, paragraph, min_size, contrast_ths, adjust_contrast, filter_ths, text_threshold, low_text, link_threshold, canvas_size, mag_ratio, slope_ths, ycenter_ths, height_ths, width_ths, y_ths, x_ths, add_margin, threshold, bbox_min_score, bbox_min_size, max_candidates, output_format)\u001b[0m\n\u001b[0;32m    466\u001b[0m \u001b[38;5;66;03m# get the 1st result from hor & free list as self.detect returns a list of depth 3\u001b[39;00m\n\u001b[0;32m    467\u001b[0m horizontal_list, free_list \u001b[38;5;241m=\u001b[39m horizontal_list[\u001b[38;5;241m0\u001b[39m], free_list[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m--> 468\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecognize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_cv_grey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhorizontal_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfree_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m    469\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mdecoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbeamWidth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m    470\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mworkers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallowlist\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mblocklist\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdetail\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrotation_info\u001b[49m\u001b[43m,\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m    471\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mparagraph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontrast_ths\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madjust_contrast\u001b[49m\u001b[43m,\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m    472\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mfilter_ths\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_ths\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_ths\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_format\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    474\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\easyocr\\easyocr.py:384\u001b[0m, in \u001b[0;36mReader.recognize\u001b[1;34m(self, img_cv_grey, horizontal_list, free_list, decoder, beamWidth, batch_size, workers, allowlist, blocklist, detail, rotation_info, paragraph, contrast_ths, adjust_contrast, filter_ths, y_ths, x_ths, reformat, output_format)\u001b[0m\n\u001b[0;32m    382\u001b[0m     f_list \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    383\u001b[0m     image_list, max_width \u001b[38;5;241m=\u001b[39m get_image_list(h_list, f_list, img_cv_grey, model_height \u001b[38;5;241m=\u001b[39m imgH)\n\u001b[1;32m--> 384\u001b[0m     result0 \u001b[38;5;241m=\u001b[39m \u001b[43mget_text\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcharacter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimgH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmax_width\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecognizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconverter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m    385\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mignore_char\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbeamWidth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontrast_ths\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madjust_contrast\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilter_ths\u001b[49m\u001b[43m,\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m    386\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mworkers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    387\u001b[0m     result \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m result0\n\u001b[0;32m    388\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m bbox \u001b[38;5;129;01min\u001b[39;00m free_list:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\easyocr\\recognition.py:218\u001b[0m, in \u001b[0;36mget_text\u001b[1;34m(character, imgH, imgW, recognizer, converter, image_list, ignore_char, decoder, beamWidth, batch_size, contrast_ths, adjust_contrast, filter_ths, workers, device)\u001b[0m\n\u001b[0;32m    214\u001b[0m     test_data \u001b[38;5;241m=\u001b[39m ListDataset(img_list2)\n\u001b[0;32m    215\u001b[0m     test_loader \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataLoader(\n\u001b[0;32m    216\u001b[0m                     test_data, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    217\u001b[0m                     num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mint\u001b[39m(workers), collate_fn\u001b[38;5;241m=\u001b[39mAlignCollate_contrast, pin_memory\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m--> 218\u001b[0m     result2 \u001b[38;5;241m=\u001b[39m \u001b[43mrecognizer_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrecognizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconverter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_max_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m    219\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43mignore_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchar_group_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbeamWidth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    221\u001b[0m result \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    222\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, zipped \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mzip\u001b[39m(coord, result1)):\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\easyocr\\recognition.py:111\u001b[0m, in \u001b[0;36mrecognizer_predict\u001b[1;34m(model, converter, test_loader, batch_max_length, ignore_idx, char_group_idx, decoder, beamWidth, device)\u001b[0m\n\u001b[0;32m    108\u001b[0m length_for_pred \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mIntTensor([batch_max_length] \u001b[38;5;241m*\u001b[39m batch_size)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m    109\u001b[0m text_for_pred \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mLongTensor(batch_size, batch_max_length \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mfill_(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m--> 111\u001b[0m preds \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_for_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;66;03m# Select max probabilty (greedy decoding) then decode index to character\u001b[39;00m\n\u001b[0;32m    114\u001b[0m preds_size \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mIntTensor([preds\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m)] \u001b[38;5;241m*\u001b[39m batch_size)\n",
      "File \u001b[1;32mc:\\Users\\Tanmay\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Tanmay\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\easyocr\\model\\vgg_model.py:30\u001b[0m, in \u001b[0;36mModel.forward\u001b[1;34m(self, input, text)\u001b[0m\n\u001b[0;32m     27\u001b[0m visual_feature \u001b[38;5;241m=\u001b[39m visual_feature\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m3\u001b[39m)\n\u001b[0;32m     29\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\" Sequence modeling stage \"\"\"\u001b[39;00m\n\u001b[1;32m---> 30\u001b[0m contextual_feature \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSequenceModeling\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvisual_feature\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\" Prediction stage \"\"\"\u001b[39;00m\n\u001b[0;32m     33\u001b[0m prediction \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mPrediction(contextual_feature\u001b[38;5;241m.\u001b[39mcontiguous())\n",
      "File \u001b[1;32mc:\\Users\\Tanmay\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Tanmay\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\Tanmay\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\container.py:240\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    238\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    239\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 240\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    241\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Tanmay\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Tanmay\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\easyocr\\model\\modules.py:98\u001b[0m, in \u001b[0;36mBidirectionalLSTM.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m: \u001b[38;5;66;03m# quantization doesn't work with this \u001b[39;00m\n\u001b[0;32m     97\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m---> 98\u001b[0m recurrent, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrnn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# batch_size x T x input_size -> batch_size x T x (2*hidden_size)\u001b[39;00m\n\u001b[0;32m     99\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear(recurrent)  \u001b[38;5;66;03m# batch_size x T x output_size\u001b[39;00m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[1;32mc:\\Users\\Tanmay\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Tanmay\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\Tanmay\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\ao\\nn\\quantized\\dynamic\\modules\\rnn.py:656\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m    654\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward_packed(\u001b[38;5;28minput\u001b[39m, hx)\n\u001b[0;32m    655\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 656\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Tanmay\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\ao\\nn\\quantized\\dynamic\\modules\\rnn.py:602\u001b[0m, in \u001b[0;36mLSTM.forward_tensor\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m    599\u001b[0m sorted_indices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    600\u001b[0m unsorted_indices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 602\u001b[0m output, hidden \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward_impl\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    603\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_sizes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_batch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msorted_indices\u001b[49m\n\u001b[0;32m    604\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    606\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpermute_hidden(hidden, unsorted_indices)\n",
      "File \u001b[1;32mc:\\Users\\Tanmay\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\ao\\nn\\quantized\\dynamic\\modules\\rnn.py:561\u001b[0m, in \u001b[0;36mLSTM.forward_impl\u001b[1;34m(self, input, hx, batch_sizes, max_batch_size, sorted_indices)\u001b[0m\n\u001b[0;32m    559\u001b[0m _all_params \u001b[38;5;241m=\u001b[39m [m\u001b[38;5;241m.\u001b[39mparam \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_all_weight_values]\n\u001b[0;32m    560\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_sizes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 561\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquantized_lstm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    562\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    563\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    564\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_all_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    565\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    566\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    567\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    568\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    569\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbidirectional\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    570\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_first\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    571\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    572\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_dynamic\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    573\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    574\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    575\u001b[0m     result \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mquantized_lstm(\n\u001b[0;32m    576\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[0;32m    577\u001b[0m         batch_sizes,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    586\u001b[0m         use_dynamic\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    587\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Tanmay\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\_ops.py:1158\u001b[0m, in \u001b[0;36mOpOverloadPacket.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1156\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_torchbind_op_overload \u001b[38;5;129;01mand\u001b[39;00m _must_dispatch_in_python(args, kwargs):\n\u001b[0;32m   1157\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _call_overload_packet_from_python(\u001b[38;5;28mself\u001b[39m, args, kwargs)\n\u001b[1;32m-> 1158\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_op\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import easyocr\n",
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import sys\n",
    "import io\n",
    "import warnings\n",
    "import logging\n",
    "import csv\n",
    "from datetime import datetime\n",
    "\n",
    "# Completely disable all warnings and logging\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"TRANSFORMERS_VERBOSITY\"] = \"error\"\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "logging.basicConfig(level=logging.ERROR)\n",
    "logging.getLogger(\"transformers\").setLevel(logging.ERROR)\n",
    "logging.getLogger(\"easyocr\").setLevel(logging.ERROR)\n",
    "\n",
    "# Suppress specific warning types\n",
    "def suppress_specific_warnings():\n",
    "    import transformers\n",
    "    transformers.logging.set_verbosity_error()\n",
    "    \n",
    "    # Patch the tokenizer warning methods\n",
    "    from transformers.tokenization_utils_base import AddedToken, BatchEncoding\n",
    "    original_init = BatchEncoding.__init__\n",
    "    def new_init(self, *args, **kwargs):\n",
    "        original_init(self, *args, **kwargs)\n",
    "    BatchEncoding.__init__ = new_init\n",
    "\n",
    "suppress_specific_warnings()\n",
    "\n",
    "# Define a context manager to suppress stdout/stderr\n",
    "class SuppressOutput:\n",
    "    def __init__(self):\n",
    "        self.stdout = None\n",
    "        self.stderr = None\n",
    "\n",
    "    def __enter__(self):\n",
    "        self.stdout = sys.stdout\n",
    "        self.stderr = sys.stderr\n",
    "        sys.stdout = io.StringIO()\n",
    "        sys.stderr = io.StringIO()\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        sys.stdout = self.stdout\n",
    "        sys.stderr = self.stderr\n",
    "\n",
    "# Define model constants\n",
    "MODEL_NAME = \"microsoft/layoutlm-base-uncased\"  # Use consistent model name\n",
    "\n",
    "# Define Paths for SROIE dataset\n",
    "task1_folder = r\"D:/Fraud Detection/data/SROIE Dataset/SROIE2019/SROIE2019/task1_train/\"\n",
    "task2_folder = r\"D:/Fraud Detection/data/SROIE Dataset/SROIE2019/SROIE2019/task2_train/\"\n",
    "output_path = r\"D:\\Fraud Detection\\outputs\"  # Folder to save the OCR results and processed images\n",
    "results_path = os.path.join(output_path, \"results\")  # Subfolder for CSV results\n",
    "\n",
    "# Ensure output directories exist\n",
    "if not os.path.exists(output_path):\n",
    "    os.makedirs(output_path)\n",
    "if not os.path.exists(results_path):\n",
    "    os.makedirs(results_path)\n",
    "\n",
    "# Step 1: OCR Processing with EasyOCR\n",
    "def perform_ocr(image_path):\n",
    "    # Initialize EasyOCR reader\n",
    "    reader = easyocr.Reader(['en'])\n",
    "    result = reader.readtext(image_path)\n",
    "    \n",
    "    # Extract bounding boxes and text\n",
    "    boxes = []\n",
    "    text = []\n",
    "    for detection in result:\n",
    "        boxes.append(detection[0])  # The coordinates of the detected text box\n",
    "        text.append(detection[1])   # The detected text\n",
    "\n",
    "    return boxes, text\n",
    "\n",
    "# Step 2: Layout Analysis (visualize OCR results)\n",
    "def visualize_ocr_results(image_path, boxes, text):\n",
    "    image = cv2.imread(image_path)\n",
    "    for box, t in zip(boxes, text):\n",
    "        pts = np.array(box, dtype=np.int32).reshape((-1, 1, 2))\n",
    "        cv2.polylines(image, [pts], isClosed=True, color=(0, 255, 0), thickness=2)\n",
    "        cv2.putText(image, t, (box[0][0], box[0][1] - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 2)\n",
    "\n",
    "    output_image_path = os.path.join(output_path, os.path.basename(image_path))\n",
    "    cv2.imwrite(output_image_path, image)\n",
    "    # plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "    # plt.axis('off')\n",
    "    # plt.show()\n",
    "\n",
    "# Step 3: Prepare data for LayoutLM (Tokenizing)\n",
    "def prepare_layoutlm_input(image_path, boxes, text):\n",
    "    # Convert image to PIL format for LayoutLM processing\n",
    "    pil_image = Image.open(image_path).convert(\"RGB\")\n",
    "    \n",
    "    # Convert EasyOCR boxes to LayoutLM format [x0, y0, x1, y1]\n",
    "    normalized_boxes = []\n",
    "    for box in boxes:\n",
    "        # Calculate min/max coordinates to get the bounding box\n",
    "        x_coordinates = [point[0] for point in box]\n",
    "        y_coordinates = [point[1] for point in box]\n",
    "        \n",
    "        x0 = int(min(x_coordinates))\n",
    "        y0 = int(min(y_coordinates))\n",
    "        x1 = int(max(x_coordinates))\n",
    "        y1 = int(max(y_coordinates))\n",
    "        \n",
    "        normalized_boxes.append([x0, y0, x1, y1])\n",
    "    \n",
    "    # For LayoutLM, we need to use a simpler approach without boxes\n",
    "    # since we're having compatibility issues with the tokenizer\n",
    "    with SuppressOutput():\n",
    "        tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "        \n",
    "        # Use a regular tokenization without layout information\n",
    "        encoding = tokenizer(\n",
    "            text,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "    \n",
    "    return encoding, pil_image, text, normalized_boxes\n",
    "\n",
    "# Step 4: LayoutLM Model Inference\n",
    "def run_layoutlm_inference(encoding):\n",
    "    # Load LayoutLM model\n",
    "    with SuppressOutput():\n",
    "        model = AutoModelForTokenClassification.from_pretrained(MODEL_NAME)\n",
    "        \n",
    "        # Perform Inference\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**encoding)\n",
    "    \n",
    "    return outputs\n",
    "\n",
    "# Step 5: Process the output\n",
    "def process_layoutlm_output(outputs):\n",
    "    # Outputs are logits, let's take the first output (assuming batch size is 1)\n",
    "    logits = outputs.logits\n",
    "    predicted_class = torch.argmax(logits, dim=-1)\n",
    "    return predicted_class\n",
    "\n",
    "# Function to save predictions to CSV\n",
    "def save_predictions(image_path, text, predicted_classes, encoding=None):\n",
    "    # Create a unique filename based on the original image and timestamp\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    base_name = os.path.splitext(os.path.basename(image_path))[0]\n",
    "    csv_filename = f\"{base_name}_{timestamp}_predictions.csv\"\n",
    "    csv_path = os.path.join(results_path, csv_filename)\n",
    "    \n",
    "    # Map predictions to words - for simplicity, just map each word to a prediction\n",
    "    results = []\n",
    "    try:\n",
    "        # Just use a simple 1:1 mapping as best we can\n",
    "        pred_list = predicted_classes[0].tolist()\n",
    "        pred_len = len(pred_list)\n",
    "        text_len = len(text)\n",
    "        \n",
    "        # Ensure number of predictions is at least the number of words\n",
    "        for i in range(min(text_len, pred_len)):\n",
    "            results.append({\n",
    "                \"word\": text[i],\n",
    "                \"predicted_class\": pred_list[i],\n",
    "                \"class_label\": \"Suspicious\" if pred_list[i] == 1 else \"Normal\"\n",
    "            })\n",
    "            \n",
    "        # If we have fewer predictions than words, assign the most common class to remaining words\n",
    "        if text_len > pred_len:\n",
    "            # Find most common class\n",
    "            from collections import Counter\n",
    "            common_class = Counter(pred_list).most_common(1)[0][0]\n",
    "            \n",
    "            for i in range(pred_len, text_len):\n",
    "                results.append({\n",
    "                    \"word\": text[i],\n",
    "                    \"predicted_class\": common_class,\n",
    "                    \"class_label\": \"Suspicious\" if common_class == 1 else \"Normal\"\n",
    "                })\n",
    "    except Exception as e:\n",
    "        # Last resort fallback - save all words with a general prediction\n",
    "        print(f\"Warning: Error mapping predictions to words: {e}\")\n",
    "        try:\n",
    "            # See if there's any positive class\n",
    "            if 1 in predicted_classes.unique().tolist():\n",
    "                overall_pred = 1\n",
    "            else:\n",
    "                overall_pred = 0\n",
    "        except:\n",
    "            # If that fails, just default to 0\n",
    "            overall_pred = 0\n",
    "            \n",
    "        for word in text:\n",
    "            results.append({\n",
    "                \"word\": word,\n",
    "                \"predicted_class\": overall_pred,\n",
    "                \"class_label\": \"Suspicious\" if overall_pred == 1 else \"Normal\"\n",
    "            })\n",
    "    \n",
    "    # Write to CSV\n",
    "    with open(csv_path, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        fieldnames = ['word', 'predicted_class', 'class_label']\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        for result in results:\n",
    "            writer.writerow(result)\n",
    "    \n",
    "    # Count suspicious words\n",
    "    suspicious_count = sum(1 for r in results if r[\"predicted_class\"] == 1)\n",
    "    total_words = len(results)\n",
    "    \n",
    "    return csv_path, suspicious_count, total_words\n",
    "\n",
    "# Function to iterate through all image files in the dataset folders\n",
    "def process_sroie_dataset():\n",
    "    # Check if dataset folders exist, otherwise use a sample image\n",
    "    dataset_exists = os.path.exists(task1_folder) or os.path.exists(task2_folder)\n",
    "    \n",
    "    if not dataset_exists:\n",
    "        print(\"Dataset folders not found. Using a sample image instead.\")\n",
    "        sample_image = r\"C:\\Users\\Tanmay\\Downloads\\crisis result.png\"\n",
    "        if os.path.exists(sample_image):\n",
    "            image_files = [sample_image]\n",
    "        else:\n",
    "            print(f\"Sample image not found. Please check the file path.\")\n",
    "            return\n",
    "    else:\n",
    "        # Collect image files from both task1_train and task2_train folders\n",
    "        image_files = []\n",
    "        for folder in [task1_folder, task2_folder]:\n",
    "            if os.path.exists(folder):\n",
    "                for root, dirs, files in os.walk(folder):\n",
    "                    for file in files:\n",
    "                        if file.endswith('.png') or file.endswith('.jpg'):\n",
    "                            image_files.append(os.path.join(root, file))\n",
    "                            \n",
    "        if not image_files:\n",
    "            print(\"No image files found in the dataset directories.\")\n",
    "            return\n",
    "    \n",
    "    for image_path in image_files:\n",
    "        print(f\"Processing: {image_path}\")\n",
    "\n",
    "        try:\n",
    "            # Step 1: Perform OCR on the image\n",
    "            boxes, text = perform_ocr(image_path)\n",
    "\n",
    "            if not text:\n",
    "                print(f\"No text detected in {image_path}, skipping...\")\n",
    "                continue\n",
    "\n",
    "            # Step 2: Visualize the OCR results\n",
    "            visualize_ocr_results(image_path, boxes, text)\n",
    "\n",
    "            # Step 3: Prepare data for LayoutLM\n",
    "            encoding, pil_image, text, normalized_boxes = prepare_layoutlm_input(image_path, boxes, text)\n",
    "\n",
    "            # Step 4: Run LayoutLM model inference\n",
    "            outputs = run_layoutlm_inference(encoding)\n",
    "\n",
    "            # Step 5: Process the LayoutLM output\n",
    "            predicted_class = process_layoutlm_output(outputs)\n",
    "            \n",
    "            # Step 6: Save predictions to CSV without displaying them in terminal\n",
    "            csv_path, suspicious_count, total_words = save_predictions(image_path, text, predicted_class, encoding)\n",
    "            \n",
    "            # Print a summary instead of raw predictions\n",
    "            if suspicious_count > 0:\n",
    "                risk_level = \"HIGH\" if suspicious_count / total_words > 0.2 else \"MEDIUM\"\n",
    "                print(f\"Result: {risk_level} RISK - {suspicious_count}/{total_words} suspicious elements\")\n",
    "            else:\n",
    "                print(f\"Result: LOW RISK - No suspicious elements found\")\n",
    "                \n",
    "            print(f\"Saved detailed analysis to: {csv_path}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {image_path}: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "\n",
    "# Main function to execute the pipeline\n",
    "def main():\n",
    "    print(\"Starting OCR and Layout Analysis Pipeline...\")\n",
    "    process_sroie_dataset()\n",
    "    print(\"OCR and Layout Analysis Complete.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting NER Processing System...\n",
      "Initializing NER Processor...\n",
      "Model loaded with 9 entity labels\n",
      "NER Processor initialized successfully!\n",
      "Looking for OCR prediction files in D:/Fraud Detection/outputs/results...\n",
      "Found 122 prediction files to process\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:   0%|          | 0/122 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing text with length: 455\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:   1%|          | 1/122 [00:00<01:59,  1.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5 entities across 2 categories\n",
      "Processing text with length: 553\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:   2%|▏         | 2/122 [00:01<01:23,  1.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3 entities across 1 categories\n",
      "Processing text with length: 706\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:   2%|▏         | 3/122 [00:02<01:24,  1.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8 entities across 2 categories\n",
      "Processing text with length: 554\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:   3%|▎         | 4/122 [00:02<01:13,  1.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 entities across 1 categories\n",
      "Processing text with length: 716\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:   4%|▍         | 5/122 [00:03<01:21,  1.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 12 entities across 1 categories\n",
      "Processing text with length: 366\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:   5%|▍         | 6/122 [00:04<01:14,  1.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5 entities across 1 categories\n",
      "Processing text with length: 863\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:   6%|▌         | 7/122 [00:04<01:19,  1.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 entities across 2 categories\n",
      "Processing text with length: 441\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:   7%|▋         | 8/122 [00:05<01:18,  1.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 7 entities across 1 categories\n",
      "Processing text with length: 853\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:   7%|▋         | 9/122 [00:06<01:20,  1.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6 entities across 2 categories\n",
      "Processing text with length: 806\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:   8%|▊         | 10/122 [00:06<01:16,  1.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 24 entities across 2 categories\n",
      "Processing text with length: 583\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:   9%|▉         | 11/122 [00:07<01:13,  1.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3 entities across 1 categories\n",
      "Processing text with length: 764\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  10%|▉         | 12/122 [00:08<01:16,  1.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3 entities across 3 categories\n",
      "Processing text with length: 566\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  11%|█         | 13/122 [00:08<01:12,  1.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4 entities across 2 categories\n",
      "Processing text with length: 515\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  11%|█▏        | 14/122 [00:09<01:09,  1.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10 entities across 2 categories\n",
      "Processing text with length: 1015\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  12%|█▏        | 15/122 [00:10<01:14,  1.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5 entities across 2 categories\n",
      "Processing text with length: 566\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  13%|█▎        | 16/122 [00:11<01:15,  1.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4 entities across 2 categories\n",
      "Processing text with length: 566\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  14%|█▍        | 17/122 [00:11<01:15,  1.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4 entities across 2 categories\n",
      "Processing text with length: 566\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  15%|█▍        | 18/122 [00:12<01:08,  1.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4 entities across 2 categories\n",
      "Processing text with length: 566\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  16%|█▌        | 19/122 [00:13<01:08,  1.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4 entities across 2 categories\n",
      "Processing text with length: 911\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  16%|█▋        | 20/122 [00:13<01:09,  1.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4 entities across 2 categories\n",
      "Processing text with length: 919\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  17%|█▋        | 21/122 [00:14<01:11,  1.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 7 entities across 1 categories\n",
      "Processing text with length: 911\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  18%|█▊        | 22/122 [00:15<01:11,  1.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4 entities across 2 categories\n",
      "Processing text with length: 540\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  19%|█▉        | 23/122 [00:15<01:05,  1.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4 entities across 2 categories\n",
      "Processing text with length: 591\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  20%|█▉        | 24/122 [00:16<01:03,  1.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4 entities across 1 categories\n",
      "Processing text with length: 663\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  20%|██        | 25/122 [00:17<01:05,  1.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5 entities across 1 categories\n",
      "Processing text with length: 507\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  21%|██▏       | 26/122 [00:17<01:01,  1.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 12 entities across 3 categories\n",
      "Processing text with length: 653\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  22%|██▏       | 27/122 [00:18<01:01,  1.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4 entities across 2 categories\n",
      "Processing text with length: 694\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  23%|██▎       | 28/122 [00:19<01:00,  1.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5 entities across 3 categories\n",
      "Processing text with length: 707\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  24%|██▍       | 29/122 [00:19<01:04,  1.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4 entities across 2 categories\n",
      "Processing text with length: 707\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  25%|██▍       | 30/122 [00:20<00:59,  1.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4 entities across 2 categories\n",
      "Processing text with length: 984\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  25%|██▌       | 31/122 [00:21<01:08,  1.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 entities across 1 categories\n",
      "Processing text with length: 984\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  26%|██▌       | 32/122 [00:22<01:13,  1.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 entities across 1 categories\n",
      "Processing text with length: 695\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  27%|██▋       | 33/122 [00:23<01:11,  1.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5 entities across 2 categories\n",
      "Processing text with length: 427\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  28%|██▊       | 34/122 [00:23<01:03,  1.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3 entities across 1 categories\n",
      "Processing text with length: 427\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  29%|██▊       | 35/122 [00:24<00:59,  1.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3 entities across 1 categories\n",
      "Processing text with length: 570\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  30%|██▉       | 36/122 [00:24<00:53,  1.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 entities across 2 categories\n",
      "Processing text with length: 570\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  30%|███       | 37/122 [00:25<00:50,  1.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 entities across 2 categories\n",
      "Processing text with length: 542\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  31%|███       | 38/122 [00:25<00:47,  1.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 12 entities across 3 categories\n",
      "Processing text with length: 542\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  32%|███▏      | 39/122 [00:26<00:50,  1.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 12 entities across 3 categories\n",
      "Processing text with length: 607\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  33%|███▎      | 40/122 [00:27<00:49,  1.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3 entities across 1 categories\n",
      "Processing text with length: 560\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  34%|███▎      | 41/122 [00:27<00:50,  1.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 9 entities across 2 categories\n",
      "Processing text with length: 561\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  34%|███▍      | 42/122 [00:28<00:50,  1.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 15 entities across 3 categories\n",
      "Processing text with length: 1247\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  35%|███▌      | 43/122 [00:29<00:55,  1.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 7 entities across 3 categories\n",
      "Processing text with length: 563\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  36%|███▌      | 44/122 [00:29<00:56,  1.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 13 entities across 3 categories\n",
      "Processing text with length: 548\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  37%|███▋      | 45/122 [00:30<00:52,  1.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10 entities across 3 categories\n",
      "Processing text with length: 557\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  38%|███▊      | 46/122 [00:31<00:47,  1.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3 entities across 1 categories\n",
      "Processing text with length: 586\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  39%|███▊      | 47/122 [00:31<00:48,  1.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 15 entities across 2 categories\n",
      "Processing text with length: 656\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  39%|███▉      | 48/122 [00:32<00:49,  1.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 23 entities across 4 categories\n",
      "Processing text with length: 438\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  40%|████      | 49/122 [00:33<00:47,  1.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3 entities across 2 categories\n",
      "Processing text with length: 452\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  41%|████      | 50/122 [00:33<00:47,  1.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 entities across 2 categories\n",
      "Processing text with length: 630\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  42%|████▏     | 51/122 [00:34<00:46,  1.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 11 entities across 1 categories\n",
      "Processing text with length: 543\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  43%|████▎     | 52/122 [00:35<00:47,  1.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6 entities across 2 categories\n",
      "Processing text with length: 602\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  43%|████▎     | 53/122 [00:35<00:46,  1.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 14 entities across 3 categories\n",
      "Processing text with length: 598\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  44%|████▍     | 54/122 [00:36<00:44,  1.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 13 entities across 3 categories\n",
      "Processing text with length: 584\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  45%|████▌     | 55/122 [00:37<00:45,  1.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3 entities across 2 categories\n",
      "Processing text with length: 192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  46%|████▌     | 56/122 [00:37<00:40,  1.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 entities across 1 categories\n",
      "Processing text with length: 723\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  47%|████▋     | 57/122 [00:38<00:41,  1.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 7 entities across 2 categories\n",
      "Processing text with length: 582\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  48%|████▊     | 58/122 [00:38<00:41,  1.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6 entities across 1 categories\n",
      "Processing text with length: 610\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  48%|████▊     | 59/122 [00:39<00:41,  1.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 21 entities across 2 categories\n",
      "Processing text with length: 575\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  49%|████▉     | 60/122 [00:40<00:42,  1.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 9 entities across 2 categories\n",
      "Processing text with length: 575\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  50%|█████     | 61/122 [00:41<00:42,  1.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 9 entities across 2 categories\n",
      "Processing text with length: 559\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  51%|█████     | 62/122 [00:41<00:42,  1.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 entities across 2 categories\n",
      "Processing text with length: 559\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  52%|█████▏    | 63/122 [00:42<00:44,  1.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 entities across 2 categories\n",
      "Processing text with length: 564\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  52%|█████▏    | 64/122 [00:43<00:41,  1.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 13 entities across 2 categories\n",
      "Processing text with length: 564\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  53%|█████▎    | 65/122 [00:44<00:40,  1.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 13 entities across 2 categories\n",
      "Processing text with length: 481\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  54%|█████▍    | 66/122 [00:44<00:36,  1.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5 entities across 3 categories\n",
      "Processing text with length: 481\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  55%|█████▍    | 67/122 [00:45<00:38,  1.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5 entities across 3 categories\n",
      "Processing text with length: 556\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  56%|█████▌    | 68/122 [00:45<00:35,  1.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 11 entities across 3 categories\n",
      "Processing text with length: 556\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  57%|█████▋    | 69/122 [00:46<00:33,  1.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 11 entities across 3 categories\n",
      "Processing text with length: 569\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  57%|█████▋    | 70/122 [00:47<00:31,  1.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 9 entities across 3 categories\n",
      "Processing text with length: 514\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  58%|█████▊    | 71/122 [00:47<00:31,  1.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 11 entities across 2 categories\n",
      "Processing text with length: 559\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  59%|█████▉    | 72/122 [00:48<00:33,  1.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 12 entities across 2 categories\n",
      "Processing text with length: 359\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  60%|█████▉    | 73/122 [00:48<00:29,  1.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 11 entities across 1 categories\n",
      "Processing text with length: 705\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  61%|██████    | 74/122 [00:49<00:29,  1.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 18 entities across 2 categories\n",
      "Processing text with length: 843\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  61%|██████▏   | 75/122 [00:50<00:31,  1.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 13 entities across 2 categories\n",
      "Processing text with length: 404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  62%|██████▏   | 76/122 [00:50<00:29,  1.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4 entities across 2 categories\n",
      "Processing text with length: 528\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  63%|██████▎   | 77/122 [00:51<00:29,  1.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 9 entities across 1 categories\n",
      "Processing text with length: 489\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  64%|██████▍   | 78/122 [00:52<00:28,  1.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 7 entities across 2 categories\n",
      "Processing text with length: 529\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  65%|██████▍   | 79/122 [00:52<00:27,  1.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4 entities across 2 categories\n",
      "Processing text with length: 685\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  66%|██████▌   | 80/122 [00:53<00:27,  1.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8 entities across 3 categories\n",
      "Processing text with length: 608\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  66%|██████▋   | 81/122 [00:54<00:27,  1.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 12 entities across 1 categories\n",
      "Processing text with length: 545\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  67%|██████▋   | 82/122 [00:54<00:26,  1.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10 entities across 2 categories\n",
      "Processing text with length: 415\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  68%|██████▊   | 83/122 [00:55<00:24,  1.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6 entities across 3 categories\n",
      "Processing text with length: 488\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  69%|██████▉   | 84/122 [00:56<00:24,  1.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 12 entities across 2 categories\n",
      "Processing text with length: 676\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  70%|██████▉   | 85/122 [00:56<00:23,  1.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8 entities across 2 categories\n",
      "Processing text with length: 863\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  70%|███████   | 86/122 [00:57<00:24,  1.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 18 entities across 4 categories\n",
      "Processing text with length: 565\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  71%|███████▏  | 87/122 [00:58<00:22,  1.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 7 entities across 1 categories\n",
      "Processing text with length: 507\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  72%|███████▏  | 88/122 [00:58<00:21,  1.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5 entities across 2 categories\n",
      "Processing text with length: 580\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  73%|███████▎  | 89/122 [00:59<00:19,  1.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 9 entities across 2 categories\n",
      "Processing text with length: 505\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  74%|███████▍  | 90/122 [00:59<00:18,  1.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 entities across 1 categories\n",
      "Processing text with length: 740\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  75%|███████▍  | 91/122 [01:00<00:19,  1.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5 entities across 1 categories\n",
      "Processing text with length: 613\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  75%|███████▌  | 92/122 [01:01<00:18,  1.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4 entities across 1 categories\n",
      "Processing text with length: 795\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  76%|███████▌  | 93/122 [01:01<00:18,  1.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4 entities across 2 categories\n",
      "Processing text with length: 589\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  77%|███████▋  | 94/122 [01:02<00:17,  1.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6 entities across 1 categories\n",
      "Processing text with length: 529\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  78%|███████▊  | 95/122 [01:03<00:18,  1.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5 entities across 3 categories\n",
      "Processing text with length: 903\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  79%|███████▊  | 96/122 [01:03<00:18,  1.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8 entities across 2 categories\n",
      "Processing text with length: 902\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  80%|███████▉  | 97/122 [01:04<00:17,  1.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 entities across 1 categories\n",
      "Processing text with length: 510\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  80%|████████  | 98/122 [01:05<00:16,  1.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10 entities across 3 categories\n",
      "Processing text with length: 541\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  81%|████████  | 99/122 [01:05<00:14,  1.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 17 entities across 2 categories\n",
      "Processing text with length: 717\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  82%|████████▏ | 100/122 [01:06<00:13,  1.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5 entities across 2 categories\n",
      "Processing text with length: 717\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  83%|████████▎ | 101/122 [01:07<00:14,  1.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5 entities across 2 categories\n",
      "Processing text with length: 510\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  84%|████████▎ | 102/122 [01:07<00:12,  1.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10 entities across 1 categories\n",
      "Processing text with length: 510\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  84%|████████▍ | 103/122 [01:08<00:11,  1.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10 entities across 1 categories\n",
      "Processing text with length: 526\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  85%|████████▌ | 104/122 [01:08<00:10,  1.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8 entities across 1 categories\n",
      "Processing text with length: 526\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  86%|████████▌ | 105/122 [01:09<00:09,  1.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8 entities across 1 categories\n",
      "Processing text with length: 549\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  87%|████████▋ | 106/122 [01:09<00:08,  1.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4 entities across 3 categories\n",
      "Processing text with length: 549\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  88%|████████▊ | 107/122 [01:10<00:08,  1.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4 entities across 3 categories\n",
      "Processing text with length: 919\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  89%|████████▊ | 108/122 [01:11<00:08,  1.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 12 entities across 2 categories\n",
      "Processing text with length: 1233\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  89%|████████▉ | 109/122 [01:12<00:08,  1.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10 entities across 2 categories\n",
      "Processing text with length: 654\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  90%|█████████ | 110/122 [01:12<00:08,  1.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 14 entities across 3 categories\n",
      "Processing text with length: 654\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  91%|█████████ | 111/122 [01:13<00:07,  1.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 14 entities across 3 categories\n",
      "Processing text with length: 654\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  92%|█████████▏| 112/122 [01:14<00:06,  1.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 14 entities across 3 categories\n",
      "Processing text with length: 654\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  93%|█████████▎| 113/122 [01:14<00:05,  1.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 14 entities across 3 categories\n",
      "Processing text with length: 654\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  93%|█████████▎| 114/122 [01:15<00:04,  1.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 14 entities across 3 categories\n",
      "Processing text with length: 624\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  94%|█████████▍| 115/122 [01:15<00:04,  1.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10 entities across 1 categories\n",
      "Processing text with length: 624\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  95%|█████████▌| 116/122 [01:16<00:03,  1.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10 entities across 1 categories\n",
      "Processing text with length: 624\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  96%|█████████▌| 117/122 [01:17<00:03,  1.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10 entities across 1 categories\n",
      "Processing text with length: 624\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  97%|█████████▋| 118/122 [01:17<00:02,  1.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10 entities across 1 categories\n",
      "Processing text with length: 610\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  98%|█████████▊| 119/122 [01:18<00:01,  1.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 12 entities across 2 categories\n",
      "Processing text with length: 610\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  98%|█████████▊| 120/122 [01:19<00:01,  1.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 12 entities across 2 categories\n",
      "Processing text with length: 610\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  99%|█████████▉| 121/122 [01:19<00:00,  1.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 12 entities across 2 categories\n",
      "Processing text with length: 610\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files: 100%|██████████| 122/122 [01:20<00:00,  1.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 12 entities across 2 categories\n",
      "Saving NER analysis results for 122 documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to D:/Fraud Detection/outputs/ner_results\n",
      "Main results file: D:/Fraud Detection/outputs/ner_results\\ner_analysis_results.csv\n",
      "\n",
      "===== NER Processing Complete =====\n",
      "Processed 122 documents\n",
      "Found 968 total entities (avg: 7.9 per document)\n",
      "Document with most entities: X51005230617 (24 entities)\n",
      "Results saved to: D:/Fraud Detection/outputs/ner_results\n",
      "NER Processing completed!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "import warnings\n",
    "import logging\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Suppress unnecessary warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "logging.getLogger(\"transformers\").setLevel(logging.ERROR)\n",
    "\n",
    "# Define paths\n",
    "RESULTS_PATH = r\"D:/Fraud Detection/outputs/results\"\n",
    "OUTPUT_PATH = r\"D:/Fraud Detection/outputs/ner_results\"\n",
    "MODEL_PATH = r\"D:/Fraud Detection/models\"\n",
    "DATA_PATH = r\"D:/Fraud Detection/data\"\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "os.makedirs(OUTPUT_PATH, exist_ok=True)\n",
    "\n",
    "# Define NER model - using a pre-trained model designed for document NER\n",
    "MODEL_NAME = \"dslim/bert-base-NER\"  # Document NER model\n",
    "\n",
    "class NERProcessor:\n",
    "    def __init__(self):\n",
    "        print(\"Initializing NER Processor...\")\n",
    "        # Load and initialize the NER model\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, cache_dir=MODEL_PATH)\n",
    "        self.model = AutoModelForTokenClassification.from_pretrained(MODEL_NAME, cache_dir=MODEL_PATH)\n",
    "        self.model.eval()  # Set model to evaluation mode\n",
    "        \n",
    "        # ID to label mapping for this NER model\n",
    "        self.id2label = self.model.config.id2label\n",
    "        print(f\"Model loaded with {len(self.id2label)} entity labels\")\n",
    "        print(\"NER Processor initialized successfully!\")\n",
    "    \n",
    "    def process_text(self, text):\n",
    "        \"\"\"Process text through NER model to extract entities\"\"\"\n",
    "        print(f\"Processing text with length: {len(text)}\")\n",
    "        \n",
    "        # Tokenize text\n",
    "        inputs = self.tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "        \n",
    "        # Run model inference\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "            predictions = torch.argmax(outputs.logits, dim=2)\n",
    "        \n",
    "        # Convert predictions to labels\n",
    "        tokens = self.tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\n",
    "        token_predictions = [self.id2label[prediction.item()] for prediction in predictions[0]]\n",
    "        \n",
    "        # Process NER results\n",
    "        word_level_predictions = []\n",
    "        current_entity = None\n",
    "        current_text = \"\"\n",
    "        \n",
    "        # Process token-level predictions to get word-level entities\n",
    "        for token, prediction in zip(tokens, token_predictions):\n",
    "            if token.startswith(\"##\"):\n",
    "                # Continuation of previous token\n",
    "                if current_entity:\n",
    "                    current_text += token[2:]  # Remove ## prefix\n",
    "            else:\n",
    "                # If we had a previous entity, add it to our list\n",
    "                if current_entity and current_text:\n",
    "                    word_level_predictions.append({\n",
    "                        \"entity\": current_entity,\n",
    "                        \"text\": current_text\n",
    "                    })\n",
    "                \n",
    "                # Start new entity\n",
    "                if prediction.startswith(\"B-\") or prediction.startswith(\"I-\"):\n",
    "                    current_entity = prediction[2:]  # Remove B- or I- prefix\n",
    "                    current_text = token\n",
    "                else:\n",
    "                    current_entity = None\n",
    "                    current_text = \"\"\n",
    "        \n",
    "        # Don't forget the last entity\n",
    "        if current_entity and current_text:\n",
    "            word_level_predictions.append({\n",
    "                \"entity\": current_entity,\n",
    "                \"text\": current_text\n",
    "            })\n",
    "        \n",
    "        # Group by entity type\n",
    "        entities = {}\n",
    "        for pred in word_level_predictions:\n",
    "            entity_type = pred[\"entity\"]\n",
    "            entity_text = pred[\"text\"]\n",
    "            \n",
    "            if entity_type not in entities:\n",
    "                entities[entity_type] = []\n",
    "            \n",
    "            entities[entity_type].append(entity_text)\n",
    "        \n",
    "        # Count the frequency of each entity\n",
    "        entity_counts = {entity: len(items) for entity, items in entities.items()}\n",
    "        \n",
    "        print(f\"Found {sum(entity_counts.values())} entities across {len(entity_counts)} categories\")\n",
    "        return entities, entity_counts\n",
    "    \n",
    "    def process_prediction_files(self):\n",
    "        \"\"\"Process all prediction files from OCR results\"\"\"\n",
    "        print(f\"Looking for OCR prediction files in {RESULTS_PATH}...\")\n",
    "        prediction_files = [f for f in os.listdir(RESULTS_PATH) if f.endswith('_predictions.csv')]\n",
    "        \n",
    "        if not prediction_files:\n",
    "            print(\"No prediction files found. Please ensure OCR has been run first.\")\n",
    "            return\n",
    "        \n",
    "        print(f\"Found {len(prediction_files)} prediction files to process\")\n",
    "        results = []\n",
    "        \n",
    "        # Process each file\n",
    "        for file in tqdm(prediction_files, desc=\"Processing files\"):\n",
    "            file_path = os.path.join(RESULTS_PATH, file)\n",
    "            \n",
    "            # Extract document ID from filename\n",
    "            doc_id = file.split('_')[0]\n",
    "            \n",
    "            try:\n",
    "                # Read OCR prediction CSV\n",
    "                df = pd.read_csv(file_path)\n",
    "                \n",
    "                # Combine all words into a single text\n",
    "                if 'word' in df.columns:\n",
    "                    text = ' '.join(df['word'].astype(str).tolist())\n",
    "                    \n",
    "                    # Process through NER\n",
    "                    entities, entity_counts = self.process_text(text)\n",
    "                    \n",
    "                    # Calculate risk metrics\n",
    "                    # More entities usually indicates a more complex document\n",
    "                    entity_complexity = len(entities)\n",
    "                    \n",
    "                    # Documents with certain entity types might have higher fraud risk\n",
    "                    high_risk_entities = ['ORG', 'MONEY', 'DATE']\n",
    "                    risk_entity_count = sum([len(entities.get(e, [])) for e in high_risk_entities])\n",
    "                    \n",
    "                    # Create result record\n",
    "                    result = {\n",
    "                        'document_id': doc_id,\n",
    "                        'total_entities': sum(entity_counts.values()),\n",
    "                        'entity_types': len(entity_counts),\n",
    "                        'entity_complexity': entity_complexity,\n",
    "                        'risk_entity_count': risk_entity_count,\n",
    "                        'entities': entities,\n",
    "                        'entity_counts': entity_counts\n",
    "                    }\n",
    "                    \n",
    "                    results.append(result)\n",
    "                else:\n",
    "                    print(f\"Warning: File {file} doesn't have the expected 'word' column\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {file}: {str(e)}\")\n",
    "        \n",
    "        # Save consolidated results\n",
    "        if results:\n",
    "            self.save_results(results)\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def save_results(self, results):\n",
    "        \"\"\"Save NER analysis results to CSV\"\"\"\n",
    "        print(f\"Saving NER analysis results for {len(results)} documents...\")\n",
    "        \n",
    "        # Create DataFrame with core metrics\n",
    "        df = pd.DataFrame([{\n",
    "            'document_id': r['document_id'],\n",
    "            'total_entities': r['total_entities'],\n",
    "            'entity_types': r['entity_types'],\n",
    "            'entity_complexity': r['entity_complexity'],\n",
    "            'risk_entity_count': r['risk_entity_count']\n",
    "        } for r in results])\n",
    "        \n",
    "        # Save to CSV\n",
    "        output_file = os.path.join(OUTPUT_PATH, \"ner_analysis_results.csv\")\n",
    "        df.to_csv(output_file, index=False)\n",
    "        \n",
    "        # Save detailed entity information for each document\n",
    "        for result in results:\n",
    "            doc_id = result['document_id']\n",
    "            entities = result['entities']\n",
    "            \n",
    "            # Create entity records\n",
    "            entity_records = []\n",
    "            for entity_type, values in entities.items():\n",
    "                for value in values:\n",
    "                    entity_records.append({\n",
    "                        'entity_type': entity_type,\n",
    "                        'value': value\n",
    "                    })\n",
    "            \n",
    "            if entity_records:\n",
    "                # Save to document-specific CSV\n",
    "                entity_df = pd.DataFrame(entity_records)\n",
    "                entity_file = os.path.join(OUTPUT_PATH, f\"{doc_id}_entities.csv\")\n",
    "                entity_df.to_csv(entity_file, index=False)\n",
    "        \n",
    "        print(f\"Results saved to {OUTPUT_PATH}\")\n",
    "        print(f\"Main results file: {os.path.join(OUTPUT_PATH, 'ner_analysis_results.csv')}\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to run NER processing\"\"\"\n",
    "    print(\"Starting NER Processing System...\")\n",
    "    processor = NERProcessor()\n",
    "    \n",
    "    # Process all OCR prediction files\n",
    "    results = processor.process_prediction_files()\n",
    "    \n",
    "    # Output summary statistics\n",
    "    if results:\n",
    "        total_entities = sum(r['total_entities'] for r in results)\n",
    "        avg_entities = total_entities / len(results)\n",
    "        max_entities = max(r['total_entities'] for r in results)\n",
    "        max_doc = next(r['document_id'] for r in results if r['total_entities'] == max_entities)\n",
    "        \n",
    "        print(\"\\n===== NER Processing Complete =====\")\n",
    "        print(f\"Processed {len(results)} documents\")\n",
    "        print(f\"Found {total_entities} total entities (avg: {avg_entities:.1f} per document)\")\n",
    "        print(f\"Document with most entities: {max_doc} ({max_entities} entities)\")\n",
    "        print(f\"Results saved to: {OUTPUT_PATH}\")\n",
    "    \n",
    "    print(\"NER Processing completed!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading NLP models...\n",
      "Downloading SpaCy model (one-time setup)...\n",
      "SpaCy model loaded successfully\n",
      "Starting Relationship Extraction System...\n",
      "Initializing Relationship Extractor...\n",
      "Found 122 documents with NER analysis\n",
      "Loaded data for 122 documents\n",
      "Relationship Extractor initialized successfully!\n",
      "\n",
      "Processing 122 documents for relationship extraction...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting relationships: 100%|██████████| 122/122 [00:03<00:00, 34.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving relationship results to D:/Fraud Detection\\outputs/relationship_results...\n",
      "\n",
      "Generating network visualizations...\n",
      "Network visualization saved to D:/Fraud Detection\\outputs/visualizations\\relationship_network.png\n",
      "\n",
      "===== Relationship Extraction Complete =====\n",
      "Processed 122 documents\n",
      "Found 80 relationships\n",
      "\n",
      "Relationship types distribution:\n",
      "  - LOCATION: 12\n",
      "  - OWNERSHIP: 68\n",
      "\n",
      "Results saved to: D:/Fraud Detection\\outputs/relationship_results\n",
      "Visualizations saved to: D:/Fraud Detection\\outputs/visualizations\n",
      "\n",
      "Relationship Extraction completed!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import json\n",
    "import spacy\n",
    "import networkx as nx\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "# Suppress unnecessary warnings and logs\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "logging.getLogger(\"matplotlib\").setLevel(logging.ERROR)\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# Define paths - ensure these are correct for your environment\n",
    "BASE_PATH = r\"D:/Fraud Detection\"\n",
    "NER_RESULTS_PATH = os.path.join(BASE_PATH, \"outputs/ner_results\")\n",
    "RELATIONSHIP_OUTPUT_PATH = os.path.join(BASE_PATH, \"outputs/relationship_results\")\n",
    "VISUALIZATION_PATH = os.path.join(BASE_PATH, \"outputs/visualizations\")\n",
    "\n",
    "# Ensure directories exist\n",
    "os.makedirs(RELATIONSHIP_OUTPUT_PATH, exist_ok=True)\n",
    "os.makedirs(VISUALIZATION_PATH, exist_ok=True)\n",
    "\n",
    "# Load spaCy model for linguistic analysis\n",
    "print(\"Loading NLP models...\")\n",
    "try:\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    print(\"SpaCy model loaded successfully\")\n",
    "except:\n",
    "    print(\"Downloading SpaCy model (one-time setup)...\")\n",
    "    os.system(\"python -m spacy download en_core_web_sm\")\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    print(\"SpaCy model loaded successfully\")\n",
    "\n",
    "class RelationshipExtractor:\n",
    "    def __init__(self):\n",
    "        print(\"Initializing Relationship Extractor...\")\n",
    "        \n",
    "        # Define relationship patterns\n",
    "        self.relationship_patterns = {\n",
    "            \"OWNERSHIP\": [\n",
    "                {\"POS\": [\"NOUN\", \"PROPN\"], \"DEP\": [\"nsubj\", \"compound\"]},\n",
    "                {\"LEMMA\": [\"own\", \"possess\", \"have\", \"hold\", \"acquire\"]},\n",
    "                {\"POS\": [\"NOUN\", \"PROPN\"], \"DEP\": [\"dobj\", \"attr\"]}\n",
    "            ],\n",
    "            \"EMPLOYMENT\": [\n",
    "                {\"POS\": [\"NOUN\", \"PROPN\"], \"DEP\": [\"nsubj\", \"compound\"]},\n",
    "                {\"LEMMA\": [\"work\", \"employ\", \"hire\", \"contract\"]},\n",
    "                {\"LEMMA\": [\"for\", \"with\", \"at\", \"by\"]},\n",
    "                {\"POS\": [\"NOUN\", \"PROPN\"], \"DEP\": [\"pobj\"]}\n",
    "            ],\n",
    "            \"TRANSACTION\": [\n",
    "                {\"POS\": [\"NOUN\", \"PROPN\"], \"DEP\": [\"nsubj\", \"compound\"]},\n",
    "                {\"LEMMA\": [\"pay\", \"transfer\", \"send\", \"receive\", \"deposit\", \"withdraw\"]},\n",
    "                {\"LEMMA\": [\"to\", \"from\"]},\n",
    "                {\"POS\": [\"NOUN\", \"PROPN\"], \"DEP\": [\"pobj\"]}\n",
    "            ],\n",
    "            \"LOCATION\": [\n",
    "                {\"POS\": [\"NOUN\", \"PROPN\"], \"DEP\": [\"nsubj\", \"compound\"]},\n",
    "                {\"LEMMA\": [\"locate\", \"situate\", \"base\", \"headquarter\", \"live\"]},\n",
    "                {\"LEMMA\": [\"in\", \"at\", \"near\"]},\n",
    "                {\"POS\": [\"NOUN\", \"PROPN\"], \"DEP\": [\"pobj\"]}\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        # Dictionary to store high-confidence relationships (these are rule-based)\n",
    "        self.high_confidence_patterns = {\n",
    "            r'(.*) is (employed|hired|working) (at|by|with) (.*)': 'EMPLOYMENT',\n",
    "            r'(.*) works for (.*)': 'EMPLOYMENT',\n",
    "            r'(.*) owns (.*)': 'OWNERSHIP', \n",
    "            r'(.*) is owned by (.*)': 'OWNERSHIP',\n",
    "            r'(.*) paid (.*) to (.*)': 'TRANSACTION',\n",
    "            r'(.*) transferred (.*) to (.*)': 'TRANSACTION',\n",
    "            r'(.*) is located (in|at) (.*)': 'LOCATION',\n",
    "            r'(.*) is based (in|at) (.*)': 'LOCATION'\n",
    "        }\n",
    "        \n",
    "        # Entity type mappings for relationships\n",
    "        self.entity_relation_mapping = {\n",
    "            ('PER', 'ORG'): ['EMPLOYMENT', 'OWNERSHIP'],\n",
    "            ('ORG', 'ORG'): ['OWNERSHIP', 'TRANSACTION'],\n",
    "            ('PER', 'PER'): ['TRANSACTION'],\n",
    "            ('ORG', 'LOC'): ['LOCATION'],\n",
    "            ('PER', 'LOC'): ['LOCATION'],\n",
    "            ('ORG', 'MONEY'): ['TRANSACTION'],\n",
    "            ('PER', 'MONEY'): ['TRANSACTION']\n",
    "        }\n",
    "        \n",
    "        # Load NER results\n",
    "        self.ner_results_file = os.path.join(NER_RESULTS_PATH, \"ner_analysis_results.csv\")\n",
    "        self.documents = self.load_ner_results()\n",
    "        print(f\"Loaded data for {len(self.documents)} documents\")\n",
    "        \n",
    "        # Prepare relationship graph\n",
    "        self.global_graph = nx.DiGraph()\n",
    "        \n",
    "        print(\"Relationship Extractor initialized successfully!\")\n",
    "    \n",
    "    def load_ner_results(self):\n",
    "        \"\"\"Load NER results from both the main CSV and individual entity files\"\"\"\n",
    "        if not os.path.exists(self.ner_results_file):\n",
    "            print(f\"ERROR: NER results file not found at {self.ner_results_file}\")\n",
    "            print(\"Please run the NER processor first.\")\n",
    "            return []\n",
    "        \n",
    "        # Load main results\n",
    "        main_df = pd.read_csv(self.ner_results_file)\n",
    "        print(f\"Found {len(main_df)} documents with NER analysis\")\n",
    "        \n",
    "        documents = []\n",
    "        for _, row in main_df.iterrows():\n",
    "            doc_id = row['document_id']\n",
    "            entity_file = os.path.join(NER_RESULTS_PATH, f\"{doc_id}_entities.csv\")\n",
    "            \n",
    "            if os.path.exists(entity_file):\n",
    "                # Load detailed entity information\n",
    "                entity_df = pd.read_csv(entity_file)\n",
    "                \n",
    "                # Group entities by type\n",
    "                entities = {}\n",
    "                for entity_type in entity_df['entity_type'].unique():\n",
    "                    entities[entity_type] = entity_df[entity_df['entity_type'] == entity_type]['value'].tolist()\n",
    "                \n",
    "                # Create document record\n",
    "                doc_record = {\n",
    "                    'document_id': doc_id,\n",
    "                    'total_entities': row['total_entities'],\n",
    "                    'entity_types': row['entity_types'],\n",
    "                    'entity_complexity': row['entity_complexity'],\n",
    "                    'risk_entity_count': row['risk_entity_count'],\n",
    "                    'entities': entities\n",
    "                }\n",
    "                documents.append(doc_record)\n",
    "        \n",
    "        return documents\n",
    "    \n",
    "    def extract_relationships_from_text(self, text, entities_by_type):\n",
    "        \"\"\"Extract relationships from text using linguistic patterns\"\"\"\n",
    "        relationships = []\n",
    "        \n",
    "        # Process text with spaCy\n",
    "        doc = nlp(text)\n",
    "        \n",
    "        # Check for rule-based high confidence patterns first\n",
    "        for pattern, rel_type in self.high_confidence_patterns.items():\n",
    "            matches = re.findall(pattern, text, re.IGNORECASE)\n",
    "            for match in matches:\n",
    "                if isinstance(match, tuple):\n",
    "                    # Multi-group match (for complex patterns)\n",
    "                    relationships.append({\n",
    "                        'type': rel_type,\n",
    "                        'source': match[0].strip(),\n",
    "                        'target': match[-1].strip(),\n",
    "                        'confidence': 'HIGH'\n",
    "                    })\n",
    "                elif isinstance(match, str):\n",
    "                    # Single group match\n",
    "                    parts = match.split(' ')\n",
    "                    if len(parts) >= 2:\n",
    "                        relationships.append({\n",
    "                            'type': rel_type,\n",
    "                            'source': parts[0].strip(),\n",
    "                            'target': ' '.join(parts[1:]).strip(),\n",
    "                            'confidence': 'HIGH'\n",
    "                        })\n",
    "        \n",
    "        # Extract relationships based on entities and their co-occurrence\n",
    "        flat_entities = {}\n",
    "        for ent_type, ents in entities_by_type.items():\n",
    "            for ent in ents:\n",
    "                flat_entities[ent.lower()] = ent_type\n",
    "        \n",
    "        # Find entity co-occurrences within sentences\n",
    "        for sent in doc.sents:\n",
    "            sent_text = sent.text.lower()\n",
    "            found_entities = []\n",
    "            \n",
    "            for entity, entity_type in flat_entities.items():\n",
    "                if entity.lower() in sent_text:\n",
    "                    found_entities.append((entity, entity_type))\n",
    "            \n",
    "            # If we have at least 2 entities in a sentence, they might be related\n",
    "            if len(found_entities) >= 2:\n",
    "                for i in range(len(found_entities)):\n",
    "                    for j in range(i+1, len(found_entities)):\n",
    "                        ent1, type1 = found_entities[i]\n",
    "                        ent2, type2 = found_entities[j]\n",
    "                        \n",
    "                        # Check if these entity types can have a relationship\n",
    "                        if (type1, type2) in self.entity_relation_mapping:\n",
    "                            possible_rels = self.entity_relation_mapping[(type1, type2)]\n",
    "                            \n",
    "                            # Use dependency parsing to infer relationship type\n",
    "                            # For now, use the first possible relationship type\n",
    "                            rel_type = possible_rels[0]\n",
    "                            \n",
    "                            relationships.append({\n",
    "                                'type': rel_type,\n",
    "                                'source': ent1,\n",
    "                                'target': ent2,\n",
    "                                'confidence': 'MEDIUM'\n",
    "                            })\n",
    "        \n",
    "        # Filter out duplicate relationships\n",
    "        unique_relationships = []\n",
    "        seen = set()\n",
    "        \n",
    "        for rel in relationships:\n",
    "            key = (rel['type'], rel['source'].lower(), rel['target'].lower())\n",
    "            if key not in seen:\n",
    "                seen.add(key)\n",
    "                unique_relationships.append(rel)\n",
    "        \n",
    "        return unique_relationships\n",
    "    \n",
    "    def process_documents(self):\n",
    "        \"\"\"Process all documents to extract relationships\"\"\"\n",
    "        print(f\"\\nProcessing {len(self.documents)} documents for relationship extraction...\")\n",
    "        \n",
    "        all_relationships = []\n",
    "        document_relationships = {}\n",
    "        \n",
    "        for doc in tqdm(self.documents, desc=\"Extracting relationships\"):\n",
    "            doc_id = doc['document_id']\n",
    "            entities = doc['entities']\n",
    "            \n",
    "            # We need text to extract relationships - let's reconstruct a simplified version\n",
    "            text = \"\"\n",
    "            for entity_type, values in entities.items():\n",
    "                for value in values:\n",
    "                    text += f\"{value} is a {entity_type}. \"\n",
    "            \n",
    "            # Extract relationships from text\n",
    "            relationships = self.extract_relationships_from_text(text, entities)\n",
    "            \n",
    "            document_relationships[doc_id] = relationships\n",
    "            all_relationships.extend(relationships)\n",
    "            \n",
    "            # Also add relationships to the global graph\n",
    "            for rel in relationships:\n",
    "                source = rel['source']\n",
    "                target = rel['target']\n",
    "                rel_type = rel['type']\n",
    "                \n",
    "                # Add nodes and edge to graph\n",
    "                self.global_graph.add_node(source)\n",
    "                self.global_graph.add_node(target)\n",
    "                \n",
    "                # Add edge with relationship type as attribute\n",
    "                self.global_graph.add_edge(source, target, type=rel_type)\n",
    "        \n",
    "        # Analyze and save relationship data\n",
    "        self.save_relationships(document_relationships, all_relationships)\n",
    "        self.visualize_relationship_network()\n",
    "        \n",
    "        # Return relationship stats\n",
    "        rel_types = Counter([rel['type'] for rel in all_relationships])\n",
    "        return {\n",
    "            'total_relationships': len(all_relationships),\n",
    "            'by_type': dict(rel_types),\n",
    "            'document_count': len(document_relationships)\n",
    "        }\n",
    "    \n",
    "    def save_relationships(self, document_relationships, all_relationships):\n",
    "        \"\"\"Save relationship extraction results\"\"\"\n",
    "        print(f\"Saving relationship results to {RELATIONSHIP_OUTPUT_PATH}...\")\n",
    "        \n",
    "        # Save all relationships to a single CSV\n",
    "        relationships_df = pd.DataFrame(all_relationships)\n",
    "        all_relationships_file = os.path.join(RELATIONSHIP_OUTPUT_PATH, \"all_relationships.csv\")\n",
    "        relationships_df.to_csv(all_relationships_file, index=False)\n",
    "        \n",
    "        # Save relationships by document\n",
    "        for doc_id, relationships in document_relationships.items():\n",
    "            if relationships:  # Only save if there are relationships\n",
    "                doc_rel_df = pd.DataFrame(relationships)\n",
    "                doc_file = os.path.join(RELATIONSHIP_OUTPUT_PATH, f\"{doc_id}_relationships.csv\")\n",
    "                doc_rel_df.to_csv(doc_file, index=False)\n",
    "        \n",
    "        # Save network data as JSON for visualization\n",
    "        network_data = {\n",
    "            'nodes': list(self.global_graph.nodes()),\n",
    "            'edges': [{'source': u, 'target': v, 'type': d['type']} \n",
    "                      for u, v, d in self.global_graph.edges(data=True)]\n",
    "        }\n",
    "        \n",
    "        network_file = os.path.join(RELATIONSHIP_OUTPUT_PATH, \"relationship_network.json\")\n",
    "        with open(network_file, 'w') as f:\n",
    "            json.dump(network_data, f)\n",
    "    \n",
    "    def visualize_relationship_network(self):\n",
    "        \"\"\"Create visualizations of the relationship network\"\"\"\n",
    "        print(\"\\nGenerating network visualizations...\")\n",
    "        \n",
    "        if len(self.global_graph) == 0:\n",
    "            print(\"No relationships found for visualization.\")\n",
    "            return\n",
    "        \n",
    "        # Set up color map for relationship types\n",
    "        rel_types = set([d['type'] for _, _, d in self.global_graph.edges(data=True)])\n",
    "        colors = plt.cm.tab10.colors\n",
    "        color_map = {rel_type: colors[i % len(colors)] for i, rel_type in enumerate(rel_types)}\n",
    "        \n",
    "        # Prepare edge colors\n",
    "        edge_colors = [color_map[self.global_graph.edges[edge]['type']] \n",
    "                      for edge in self.global_graph.edges()]\n",
    "        \n",
    "        # Create main visualization\n",
    "        plt.figure(figsize=(14, 10))\n",
    "        pos = nx.spring_layout(self.global_graph, seed=42)\n",
    "        \n",
    "        # Draw network\n",
    "        nx.draw_networkx_nodes(self.global_graph, pos, alpha=0.8, node_size=500)\n",
    "        nx.draw_networkx_labels(self.global_graph, pos, font_size=10)\n",
    "        nx.draw_networkx_edges(self.global_graph, pos, width=2, alpha=0.7, edge_color=edge_colors)\n",
    "        \n",
    "        # Add legend\n",
    "        for rel_type, color in color_map.items():\n",
    "            plt.plot([0], [0], color=color, label=rel_type, linewidth=3)\n",
    "        \n",
    "        plt.legend(title=\"Relationship Types\", loc=\"upper right\")\n",
    "        plt.title(\"Entity Relationship Network\", size=15)\n",
    "        plt.axis('off')\n",
    "        \n",
    "        # Save visualization\n",
    "        network_viz_file = os.path.join(VISUALIZATION_PATH, \"relationship_network.png\")\n",
    "        plt.savefig(network_viz_file, dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        # Create additional stats visualizations\n",
    "        self.create_stats_visualizations()\n",
    "        \n",
    "        print(f\"Network visualization saved to {network_viz_file}\")\n",
    "    \n",
    "    def create_stats_visualizations(self):\n",
    "        \"\"\"Create additional visualizations for relationship statistics\"\"\"\n",
    "        # Count relationship types\n",
    "        rel_types = Counter([d['type'] for _, _, d in self.global_graph.edges(data=True)])\n",
    "        \n",
    "        # Relationship types distribution\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.bar(rel_types.keys(), rel_types.values(), color=plt.cm.tab10.colors[:len(rel_types)])\n",
    "        plt.title(\"Distribution of Relationship Types\", size=15)\n",
    "        plt.xlabel(\"Relationship Type\")\n",
    "        plt.ylabel(\"Count\")\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(VISUALIZATION_PATH, \"relationship_types.png\"), dpi=300)\n",
    "        plt.close()\n",
    "        \n",
    "        # Node connectivity (degree)\n",
    "        node_degrees = dict(self.global_graph.degree())\n",
    "        top_nodes = sorted(node_degrees.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "        \n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.bar([n[0] for n in top_nodes], [n[1] for n in top_nodes], \n",
    "                color=plt.cm.viridis(np.linspace(0, 1, len(top_nodes))))\n",
    "        plt.title(\"Top 10 Most Connected Entities\", size=15)\n",
    "        plt.xlabel(\"Entity\")\n",
    "        plt.ylabel(\"Number of Connections\")\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(VISUALIZATION_PATH, \"top_connected_entities.png\"), dpi=300)\n",
    "        plt.close()\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to run relationship extraction\"\"\"\n",
    "    print(\"Starting Relationship Extraction System...\")\n",
    "    \n",
    "    extractor = RelationshipExtractor()\n",
    "    \n",
    "    # Process all documents\n",
    "    results = extractor.process_documents()\n",
    "    \n",
    "    # Print summary statistics\n",
    "    print(\"\\n===== Relationship Extraction Complete =====\")\n",
    "    print(f\"Processed {results['document_count']} documents\")\n",
    "    print(f\"Found {results['total_relationships']} relationships\")\n",
    "    print(\"\\nRelationship types distribution:\")\n",
    "    for rel_type, count in results['by_type'].items():\n",
    "        print(f\"  - {rel_type}: {count}\")\n",
    "    \n",
    "    print(f\"\\nResults saved to: {RELATIONSHIP_OUTPUT_PATH}\")\n",
    "    print(f\"Visualizations saved to: {VISUALIZATION_PATH}\")\n",
    "    \n",
    "    print(\"\\nRelationship Extraction completed!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-14 11:24:28.608 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-14 11:24:28.612 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-14 11:24:28.614 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-14 11:24:28.709 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-14 11:24:28.709 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-14 11:24:28.710 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-14 11:24:28.711 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-14 11:24:28.712 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-14 11:24:28.712 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-14 11:24:28.713 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-14 11:24:28.714 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-14 11:24:28.714 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-14 11:24:28.714 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-14 11:24:28.793 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-14 11:24:28.794 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-14 11:24:28.795 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-14 11:24:28.795 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-14 11:24:28.797 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-14 11:24:28.797 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-14 11:24:28.809 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-14 11:24:28.810 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
     ]
    }
   ],
   "source": [
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# Set page config\n",
    "st.set_page_config(\n",
    "    page_title=\"Fraud Detection Dashboard\",\n",
    "    page_icon=\"🔍\",\n",
    "    layout=\"wide\"\n",
    ")\n",
    "\n",
    "# Constants - Updated paths to be more flexible\n",
    "# Constants - Updated paths to be more flexible\n",
    "BASE_PATH = r\"D:/Fraud Detection\"  # Direct path for notebook compatibility\n",
    "RELATIONSHIP_OUTPUT_PATH = os.path.join(BASE_PATH, \"outputs\", \"relationship_results\")\n",
    "NER_RESULTS_PATH = os.path.join(BASE_PATH, \"outputs\", \"ner_results\")\n",
    "# Ensure output directories exist\n",
    "os.makedirs(RELATIONSHIP_OUTPUT_PATH, exist_ok=True)\n",
    "os.makedirs(NER_RESULTS_PATH, exist_ok=True)\n",
    "\n",
    "class FraudDetectionDashboard:\n",
    "    def __init__(self):\n",
    "        self.load_data()\n",
    "        \n",
    "    def load_data(self):\n",
    "        \"\"\"Load all necessary data for the dashboard\"\"\"\n",
    "        # Load relationship data\n",
    "        self.relationships = self._load_relationship_data()\n",
    "        \n",
    "        # Load NER results\n",
    "        self.ner_results = self._load_ner_results()\n",
    "        \n",
    "        # Create network graph\n",
    "        self.G = self._create_network_graph()\n",
    "        \n",
    "    def _load_relationship_data(self):\n",
    "        \"\"\"Load relationship data from CSV files\"\"\"\n",
    "        relationships = []\n",
    "        if os.path.exists(RELATIONSHIP_OUTPUT_PATH):\n",
    "            for file in os.listdir(RELATIONSHIP_OUTPUT_PATH):\n",
    "                if file.endswith('_relationships.csv'):\n",
    "                    df = pd.read_csv(os.path.join(RELATIONSHIP_OUTPUT_PATH, file))\n",
    "                    relationships.append(df)\n",
    "        return pd.concat(relationships) if relationships else pd.DataFrame()\n",
    "    \n",
    "    def _load_ner_results(self):\n",
    "        \"\"\"Load NER results from CSV files\"\"\"\n",
    "        ner_results = []\n",
    "        if os.path.exists(NER_RESULTS_PATH):\n",
    "            for file in os.listdir(NER_RESULTS_PATH):\n",
    "                if file.endswith('_entities.csv'):\n",
    "                    df = pd.read_csv(os.path.join(NER_RESULTS_PATH, file))\n",
    "                    ner_results.append(df)\n",
    "        return pd.concat(ner_results) if ner_results else pd.DataFrame()\n",
    "    \n",
    "    def _create_network_graph(self):\n",
    "        \"\"\"Create a network graph from relationships\"\"\"\n",
    "        G = nx.DiGraph()\n",
    "        \n",
    "        if not self.relationships.empty:\n",
    "            for _, row in self.relationships.iterrows():\n",
    "                G.add_edge(\n",
    "                    row['source'],\n",
    "                    row['target'],\n",
    "                    relationship_type=row['type'],\n",
    "                    confidence=row['confidence']\n",
    "                )\n",
    "        \n",
    "        return G\n",
    "    \n",
    "    def plot_network_graph(self):\n",
    "        \"\"\"Create an interactive network graph using plotly\"\"\"\n",
    "        if not self.G.edges():\n",
    "            st.warning(\"No relationships found to visualize.\")\n",
    "            return None\n",
    "        \n",
    "        # Create node positions using spring layout\n",
    "        pos = nx.spring_layout(self.G)\n",
    "        \n",
    "        # Create edge trace\n",
    "        edge_x = []\n",
    "        edge_y = []\n",
    "        edge_text = []\n",
    "        \n",
    "        for edge in self.G.edges(data=True):\n",
    "            x0, y0 = pos[edge[0]]\n",
    "            x1, y1 = pos[edge[1]]\n",
    "            edge_x.extend([x0, x1, None])\n",
    "            edge_y.extend([y0, y1, None])\n",
    "            edge_text.append(f\"{edge[2]['relationship_type']} ({edge[2]['confidence']})\")\n",
    "        \n",
    "        edge_trace = go.Scatter(\n",
    "            x=edge_x, y=edge_y,\n",
    "            line=dict(width=0.5, color='#888'),\n",
    "            hoverinfo='text',\n",
    "            mode='lines',\n",
    "            text=edge_text\n",
    "        )\n",
    "        \n",
    "        # Create node trace\n",
    "        node_x = []\n",
    "        node_y = []\n",
    "        node_text = []\n",
    "        node_connections = []\n",
    "        \n",
    "        for node in self.G.nodes():\n",
    "            x, y = pos[node]\n",
    "            node_x.append(x)\n",
    "            node_y.append(y)\n",
    "            node_text.append(node)\n",
    "            node_connections.append(len(list(self.G.neighbors(node))))\n",
    "        \n",
    "        node_trace = go.Scatter(\n",
    "            x=node_x, y=node_y,\n",
    "            mode='markers+text',\n",
    "            hoverinfo='text',\n",
    "            text=node_text,\n",
    "            textposition=\"top center\",\n",
    "            marker=dict(\n",
    "                showscale=True,\n",
    "                colorscale='YlGnBu',\n",
    "                size=10,\n",
    "                color=node_connections,\n",
    "                colorbar=dict(\n",
    "                    thickness=15,\n",
    "                    title='Node Connections',\n",
    "                    xanchor='left'\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        # Create figure with updated layout\n",
    "        fig = go.Figure(data=[edge_trace, node_trace])\n",
    "        fig.update_layout(\n",
    "            title='Relationship Network',\n",
    "            showlegend=False,\n",
    "            hovermode='closest',\n",
    "            margin=dict(b=20,l=5,r=5,t=40),\n",
    "            xaxis=dict(showgrid=False, zeroline=False, showticklabels=False),\n",
    "            yaxis=dict(showgrid=False, zeroline=False, showticklabels=False)\n",
    "        )\n",
    "        \n",
    "        return fig\n",
    "    \n",
    "    def get_relationship_stats(self):\n",
    "        \"\"\"Calculate and return relationship statistics\"\"\"\n",
    "        if self.relationships.empty:\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        stats = {\n",
    "            'Total Relationships': len(self.relationships),\n",
    "            'Unique Entities': len(set(self.relationships['source'].unique()) | set(self.relationships['target'].unique())),\n",
    "            'Relationship Types': self.relationships['type'].nunique(),\n",
    "            'High Confidence Relationships': len(self.relationships[self.relationships['confidence'] == 'HIGH']),\n",
    "            'Average Relationships per Entity': len(self.relationships) / len(set(self.relationships['source'].unique()) | set(self.relationships['target'].unique()))\n",
    "        }\n",
    "        \n",
    "        return pd.DataFrame(list(stats.items()), columns=['Metric', 'Value'])\n",
    "    \n",
    "    def get_entity_risk_scores(self):\n",
    "        \"\"\"Calculate risk scores for entities based on their relationships\"\"\"\n",
    "        if self.relationships.empty:\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        # Calculate risk scores based on relationship types and confidence\n",
    "        risk_scores = {}\n",
    "        for _, row in self.relationships.iterrows():\n",
    "            source = row['source']\n",
    "            target = row['target']\n",
    "            rel_type = row['type']\n",
    "            confidence = row['confidence']\n",
    "            \n",
    "            # Assign risk weights based on relationship type\n",
    "            risk_weights = {\n",
    "                'TRANSACTION': 2.0,\n",
    "                'OWNERSHIP': 1.5,\n",
    "                'EMPLOYMENT': 1.0,\n",
    "                'LOCATION': 0.5\n",
    "            }\n",
    "            \n",
    "            # Calculate base risk\n",
    "            base_risk = risk_weights.get(rel_type, 1.0)\n",
    "            \n",
    "            # Adjust for confidence\n",
    "            confidence_multiplier = 1.5 if confidence == 'HIGH' else 1.0\n",
    "            \n",
    "            # Update risk scores\n",
    "            risk_scores[source] = risk_scores.get(source, 0) + (base_risk * confidence_multiplier)\n",
    "            risk_scores[target] = risk_scores.get(target, 0) + (base_risk * confidence_multiplier)\n",
    "        \n",
    "        # Convert to DataFrame\n",
    "        risk_df = pd.DataFrame(list(risk_scores.items()), columns=['Entity', 'Risk Score'])\n",
    "        risk_df = risk_df.sort_values('Risk Score', ascending=False)\n",
    "        \n",
    "        return risk_df\n",
    "\n",
    "def main():\n",
    "    st.title(\"🔍 Fraud Detection Dashboard\")\n",
    "    \n",
    "    # Initialize dashboard\n",
    "    dashboard = FraudDetectionDashboard()\n",
    "    \n",
    "    # Sidebar\n",
    "    st.sidebar.title(\"Dashboard Controls\")\n",
    "    view_option = st.sidebar.selectbox(\n",
    "        \"Select View\",\n",
    "        [\"Network Graph\", \"Risk Analysis\", \"Entity Analysis\"]\n",
    "    )\n",
    "    \n",
    "    # Main content\n",
    "    if view_option == \"Network Graph\":\n",
    "        st.header(\"Relationship Network Visualization\")\n",
    "        fig = dashboard.plot_network_graph()\n",
    "        if fig is not None:\n",
    "            st.plotly_chart(fig, use_container_width=True)\n",
    "        \n",
    "        # Show relationship statistics\n",
    "        st.subheader(\"Relationship Statistics\")\n",
    "        stats_df = dashboard.get_relationship_stats()\n",
    "        st.dataframe(stats_df, use_container_width=True)\n",
    "        \n",
    "    elif view_option == \"Risk Analysis\":\n",
    "        st.header(\"Entity Risk Analysis\")\n",
    "        risk_df = dashboard.get_entity_risk_scores()\n",
    "        \n",
    "        if not risk_df.empty:\n",
    "            # Plot risk scores\n",
    "            fig = px.bar(risk_df.head(10), \n",
    "                        x='Entity', \n",
    "                        y='Risk Score',\n",
    "                        title='Top 10 High-Risk Entities')\n",
    "            st.plotly_chart(fig, use_container_width=True)\n",
    "            \n",
    "            # Show detailed risk scores\n",
    "            st.subheader(\"Detailed Risk Scores\")\n",
    "            st.dataframe(risk_df, use_container_width=True)\n",
    "        else:\n",
    "            st.warning(\"No risk analysis data available.\")\n",
    "        \n",
    "    else:  # Entity Analysis\n",
    "        st.header(\"Entity Analysis\")\n",
    "        \n",
    "        if not dashboard.ner_results.empty:\n",
    "            # Entity type distribution\n",
    "            entity_types = dashboard.ner_results['entity_type'].value_counts()\n",
    "            fig = px.pie(values=entity_types.values, \n",
    "                        names=entity_types.index,\n",
    "                        title='Entity Type Distribution')\n",
    "            st.plotly_chart(fig, use_container_width=True)\n",
    "            \n",
    "            # Show entity details\n",
    "            st.subheader(\"Entity Details\")\n",
    "            st.dataframe(dashboard.ner_results, use_container_width=True)\n",
    "        else:\n",
    "            st.warning(\"No entity data available for analysis.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running command: c:\\Users\\Tanmay\\AppData\\Local\\Programs\\Python\\Python311\\python.exe -m streamlit run d:\\Fraud Detection\\Scripts\\dashboard.py\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def run_dashboard():\n",
    "    try:\n",
    "        # Get the directory of the current script, fallback to cwd if __file__ is not defined\n",
    "        current_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "    except NameError:\n",
    "        current_dir = os.getcwd()\n",
    "    \n",
    "    # Path to the dashboard script\n",
    "    dashboard_path = os.path.join(current_dir, \"dashboard.py\")\n",
    "    \n",
    "    # Ensure we're in the correct directory\n",
    "    os.chdir(current_dir)\n",
    "    \n",
    "    # Run the dashboard using streamlit\n",
    "    try:\n",
    "        # Use the full path to streamlit\n",
    "        streamlit_cmd = [sys.executable, \"-m\", \"streamlit\", \"run\", dashboard_path]\n",
    "        print(f\"Running command: {' '.join(streamlit_cmd)}\")\n",
    "        subprocess.run(streamlit_cmd, check=True)\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Error running dashboard: {e}\")\n",
    "        print(\"Make sure you have installed all required dependencies:\")\n",
    "        print(\"pip install -r requirements.txt\")\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_dashboard()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tanmay\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Neither CUDA nor MPS are available - defaulting to CPU. Note: This module is much faster with a GPU.\n",
      "Some weights of LayoutLMForSequenceClassification were not initialized from the model checkpoint at microsoft/layoutlm-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2025-05-16 02:25:01.737 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-16 02:25:03.185 \n",
      "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
      "  command:\n",
      "\n",
      "    streamlit run C:\\Users\\Tanmay\\AppData\\Roaming\\Python\\Python311\\site-packages\\ipykernel_launcher.py [ARGUMENTS]\n",
      "2025-05-16 02:25:03.186 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-16 02:25:03.188 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-16 02:25:03.189 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-16 02:25:03.190 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-16 02:25:03.191 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-16 02:25:03.192 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-16 02:25:03.193 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-16 02:25:03.193 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-16 02:25:03.194 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-16 02:25:03.195 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
     ]
    }
   ],
   "source": [
    "import streamlit as st\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import easyocr\n",
    "import torch\n",
    "from transformers import LayoutLMForSequenceClassification, LayoutLMTokenizer\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import tempfile\n",
    "import os\n",
    "\n",
    "# Initialize EasyOCR reader\n",
    "reader = easyocr.Reader(['en'])\n",
    "\n",
    "# Initialize LayoutLM model and tokenizer\n",
    "model_name = \"microsoft/layoutlm-base-uncased\"\n",
    "tokenizer = LayoutLMTokenizer.from_pretrained(model_name)\n",
    "model = LayoutLMForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "def process_image(image):\n",
    "    \"\"\"Process the uploaded image and extract text using OCR\"\"\"\n",
    "    # Convert image to numpy array\n",
    "    img_array = np.array(image)\n",
    "    \n",
    "    # Perform OCR\n",
    "    results = reader.readtext(img_array)\n",
    "    \n",
    "    # Extract text and bounding boxes\n",
    "    extracted_text = []\n",
    "    boxes = []\n",
    "    for (bbox, text, prob) in results:\n",
    "        if prob > 0.5:  # Confidence threshold\n",
    "            extracted_text.append(text)\n",
    "            boxes.append(bbox)\n",
    "    \n",
    "    return extracted_text, boxes, img_array\n",
    "\n",
    "def analyze_layout(text, boxes, image):\n",
    "    \"\"\"Analyze the document layout using LayoutLM\"\"\"\n",
    "    # Prepare input for LayoutLM\n",
    "    encoding = tokenizer(\n",
    "        text,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=512\n",
    "    )\n",
    "    \n",
    "    # Get model predictions\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**encoding)\n",
    "        predictions = outputs.logits.argmax(-1)\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "def detect_fraud(text, layout_analysis):\n",
    "    \"\"\"Perform fraud detection based on extracted text and layout analysis\"\"\"\n",
    "    # This is a simplified example - you should implement your actual fraud detection logic here\n",
    "    fraud_indicators = {\n",
    "        'amount_mismatch': False,\n",
    "        'suspicious_patterns': False,\n",
    "        'layout_anomalies': False\n",
    "    }\n",
    "    \n",
    "    # Example checks (customize based on your requirements)\n",
    "    if any('$' in t for t in text):\n",
    "        amounts = [float(t.replace('$', '')) for t in text if '$' in t]\n",
    "        if len(amounts) > 1 and max(amounts) - min(amounts) > 100:\n",
    "            fraud_indicators['amount_mismatch'] = True\n",
    "    \n",
    "    if layout_analysis is not None and torch.any(layout_analysis != 0):\n",
    "        fraud_indicators['layout_anomalies'] = True\n",
    "    \n",
    "    return fraud_indicators\n",
    "\n",
    "def main():\n",
    "    st.title(\"Document Fraud Detection System\")\n",
    "    st.write(\"Upload a document image to detect potential fraud\")\n",
    "\n",
    "    # File uploader\n",
    "    uploaded_file = st.file_uploader(\"Choose an image file\", type=['png', 'jpg', 'jpeg'])\n",
    "\n",
    "    if uploaded_file is not None:\n",
    "        # Display the uploaded image\n",
    "        image = Image.open(uploaded_file)\n",
    "        st.image(image, caption='Uploaded Document', use_column_width=True)\n",
    "\n",
    "        # Process the image\n",
    "        with st.spinner('Processing document...'):\n",
    "            # Extract text and boxes\n",
    "            extracted_text, boxes, img_array = process_image(image)\n",
    "            \n",
    "            # Analyze layout\n",
    "            layout_analysis = analyze_layout(extracted_text, boxes, img_array)\n",
    "            \n",
    "            # Detect fraud\n",
    "            fraud_indicators = detect_fraud(extracted_text, layout_analysis)\n",
    "\n",
    "            # Display results\n",
    "            st.subheader(\"Extracted Text\")\n",
    "            st.write(extracted_text)\n",
    "\n",
    "            st.subheader(\"Fraud Detection Results\")\n",
    "            \n",
    "            # Create a DataFrame for visualization\n",
    "            fraud_df = pd.DataFrame({\n",
    "                'Indicator': list(fraud_indicators.keys()),\n",
    "                'Detected': list(fraud_indicators.values())\n",
    "            })\n",
    "            \n",
    "            # Create a bar chart\n",
    "            fig = px.bar(fraud_df, x='Indicator', y='Detected',\n",
    "                        title='Fraud Detection Indicators',\n",
    "                        color='Detected',\n",
    "                        color_discrete_map={True: 'red', False: 'green'})\n",
    "            st.plotly_chart(fig)\n",
    "\n",
    "            # Display detailed results\n",
    "            st.subheader(\"Detailed Analysis\")\n",
    "            for indicator, detected in fraud_indicators.items():\n",
    "                status = \"⚠️ Detected\" if detected else \"✅ Not Detected\"\n",
    "                st.write(f\"{indicator.replace('_', ' ').title()}: {status}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Neither CUDA nor MPS are available - defaulting to CPU. Note: This module is much faster with a GPU.\n",
      "Some weights of LayoutLMForSequenceClassification were not initialized from the model checkpoint at microsoft/layoutlm-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2025-05-16 04:00:28.040 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-16 04:00:28.041 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-16 04:00:28.042 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-16 04:00:28.042 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-16 04:00:28.044 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-16 04:00:28.044 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-16 04:00:28.044 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-16 04:00:28.045 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-16 04:00:28.046 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-16 04:00:28.046 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-16 04:00:28.047 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
     ]
    }
   ],
   "source": [
    "import streamlit as st\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import easyocr\n",
    "import torch\n",
    "from transformers import LayoutLMForSequenceClassification, LayoutLMTokenizer\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import tempfile\n",
    "import os\n",
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "# Initialize EasyOCR reader\n",
    "reader = easyocr.Reader(['en'])\n",
    "\n",
    "# Initialize LayoutLM model and tokenizer\n",
    "model_name = \"microsoft/layoutlm-base-uncased\"\n",
    "tokenizer = LayoutLMTokenizer.from_pretrained(model_name)\n",
    "model = LayoutLMForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "def process_image(image):\n",
    "    \"\"\"Process the uploaded image and extract text using OCR\"\"\"\n",
    "    # Convert image to numpy array\n",
    "    img_array = np.array(image)\n",
    "    \n",
    "    # Perform OCR\n",
    "    results = reader.readtext(img_array)\n",
    "    \n",
    "    # Extract text and bounding boxes\n",
    "    extracted_text = []\n",
    "    boxes = []\n",
    "    for (bbox, text, prob) in results:\n",
    "        if prob > 0.5:  # Confidence threshold\n",
    "            extracted_text.append(text)\n",
    "            boxes.append(bbox)\n",
    "    \n",
    "    return extracted_text, boxes, img_array\n",
    "\n",
    "def analyze_layout(text, boxes, image):\n",
    "    \"\"\"Analyze the document layout using LayoutLM\"\"\"\n",
    "    # Prepare input for LayoutLM\n",
    "    encoding = tokenizer(\n",
    "        text,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=512\n",
    "    )\n",
    "    \n",
    "    # Get model predictions\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**encoding)\n",
    "        predictions = outputs.logits.argmax(-1)\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "def extract_amounts(text):\n",
    "    \"\"\"Extract monetary amounts from text\"\"\"\n",
    "    amounts = []\n",
    "    # Match patterns like $100, 100.00, $1,234.56\n",
    "    amount_pattern = r'\\$?\\d{1,3}(?:,\\d{3})*(?:\\.\\d{2})?'\n",
    "    for t in text:\n",
    "        matches = re.findall(amount_pattern, t)\n",
    "        for match in matches:\n",
    "            try:\n",
    "                # Remove $ and , characters and convert to float\n",
    "                clean_amount = float(match.replace('$', '').replace(',', ''))\n",
    "                amounts.append(clean_amount)\n",
    "            except ValueError:\n",
    "                continue\n",
    "    return amounts\n",
    "\n",
    "def extract_dates(text):\n",
    "    \"\"\"Extract dates from text\"\"\"\n",
    "    dates = []\n",
    "    # Common date patterns\n",
    "    date_patterns = [\n",
    "        r'\\d{1,2}/\\d{1,2}/\\d{2,4}',  # MM/DD/YYYY\n",
    "        r'\\d{1,2}-\\d{1,2}-\\d{2,4}',  # MM-DD-YYYY\n",
    "        r'\\d{1,2}\\.\\d{1,2}\\.\\d{2,4}'  # MM.DD.YYYY\n",
    "    ]\n",
    "    \n",
    "    for t in text:\n",
    "        for pattern in date_patterns:\n",
    "            matches = re.findall(pattern, t)\n",
    "            dates.extend(matches)\n",
    "    return dates\n",
    "\n",
    "def detect_fraud(text, layout_analysis):\n",
    "    \"\"\"Perform comprehensive fraud detection based on extracted text and layout analysis\"\"\"\n",
    "    fraud_indicators = {\n",
    "        'amount_mismatch': False,\n",
    "        'suspicious_patterns': False,\n",
    "        'layout_anomalies': False,\n",
    "        'date_inconsistency': False,\n",
    "        'amount_format_anomaly': False,\n",
    "        'missing_crucial_info': False,\n",
    "        'duplicate_amounts': False\n",
    "    }\n",
    "\n",
    "    # --- Amount Analysis ---\n",
    "    amounts = extract_amounts(text)\n",
    "    if amounts:\n",
    "        total_amount = max(amounts)\n",
    "        line_items = [amt for amt in amounts if amt != total_amount]\n",
    "        line_sum = sum(line_items)\n",
    "\n",
    "        # Allow for a 5% mismatch (due to rounding, taxes, etc.)\n",
    "        if line_items and abs(line_sum - total_amount) / total_amount > 0.05:\n",
    "            fraud_indicators['amount_mismatch'] = True\n",
    "\n",
    "        if len(set(amounts)) != len(amounts):\n",
    "            fraud_indicators['duplicate_amounts'] = True\n",
    "\n",
    "        if any(amt < 0 for amt in amounts):\n",
    "            fraud_indicators['amount_format_anomaly'] = True\n",
    "\n",
    "    # --- Date Analysis ---\n",
    "    dates = extract_dates(text)\n",
    "    parsed_dates = []\n",
    "    for d in dates:\n",
    "        for fmt in ['%m/%d/%Y', '%m-%d-%Y', '%m.%d.%Y', '%m/%d/%y', '%m-%d-%y', '%m.%d.%y']:\n",
    "            try:\n",
    "                parsed = datetime.strptime(d, fmt)\n",
    "                parsed_dates.append(parsed)\n",
    "                break\n",
    "            except ValueError:\n",
    "                continue\n",
    "    if len(parsed_dates) >= 2:\n",
    "        if not all(parsed_dates[i] <= parsed_dates[i+1] for i in range(len(parsed_dates)-1)):\n",
    "            fraud_indicators['date_inconsistency'] = True\n",
    "\n",
    "    # --- Layout Anomalies ---\n",
    "    if layout_analysis is not None and layout_analysis.item() != 0:\n",
    "        fraud_indicators['layout_anomalies'] = True\n",
    "\n",
    "    # --- Missing Information ---\n",
    "    crucial_keywords = ['total', 'date', 'invoice', 'bill', 'amount']\n",
    "    text_lower = ' '.join(text).lower()\n",
    "    missing = [word for word in crucial_keywords if word not in text_lower]\n",
    "    if len(missing) >= 3:\n",
    "        fraud_indicators['missing_crucial_info'] = True\n",
    "\n",
    "    # --- Suspicious Patterns ---\n",
    "    suspicious_patterns = [\n",
    "        r'\\d{16}',               # credit card\n",
    "        r'\\d{3}-\\d{2}-\\d{4}',    # SSN\n",
    "        r'\\bvoid\\b',             # Voided\n",
    "        r'\\bcopy\\b',             # Copy\n",
    "        r'\\bduplicate\\b'         # Duplicate\n",
    "    ]\n",
    "    if any(re.search(pat, ' '.join(text), re.IGNORECASE) for pat in suspicious_patterns):\n",
    "        fraud_indicators['suspicious_patterns'] = True\n",
    "\n",
    "    return fraud_indicators\n",
    "\n",
    "\n",
    "def main():\n",
    "    st.title(\"Document Fraud Detection System\")\n",
    "    st.write(\"Upload a document image to detect potential fraud\")\n",
    "\n",
    "    # File uploader\n",
    "    uploaded_file = st.file_uploader(\"Choose an image file\", type=['png', 'jpg', 'jpeg'])\n",
    "\n",
    "    if uploaded_file is not None:\n",
    "        # Display the uploaded image\n",
    "        image = Image.open(uploaded_file)\n",
    "        st.image(image, caption='Uploaded Document', use_column_width=True)\n",
    "\n",
    "        # Process the image\n",
    "        with st.spinner('Processing document...'):\n",
    "            # Extract text and boxes\n",
    "            extracted_text, boxes, img_array = process_image(image)\n",
    "            \n",
    "            # Analyze layout\n",
    "            layout_analysis = analyze_layout(extracted_text, boxes, img_array)\n",
    "            \n",
    "            # Detect fraud\n",
    "            fraud_indicators = detect_fraud(extracted_text, layout_analysis)\n",
    "\n",
    "            # Display results\n",
    "            st.subheader(\"Extracted Text\")\n",
    "            st.write(extracted_text)\n",
    "\n",
    "            st.subheader(\"Fraud Detection Results\")\n",
    "            \n",
    "            # Create a DataFrame for visualization\n",
    "            fraud_df = pd.DataFrame({\n",
    "                'Indicator': list(fraud_indicators.keys()),\n",
    "                'Detected': list(fraud_indicators.values())\n",
    "            })\n",
    "            \n",
    "            # Create a bar chart\n",
    "            fig = px.bar(fraud_df, x='Indicator', y='Detected',\n",
    "                        title='Fraud Detection Indicators',\n",
    "                        color='Detected',\n",
    "                        color_discrete_map={True: 'red', False: 'green'})\n",
    "            st.plotly_chart(fig)\n",
    "\n",
    "            # Display detailed results\n",
    "            st.subheader(\"Detailed Analysis\")\n",
    "            for indicator, detected in fraud_indicators.items():\n",
    "                status = \"⚠️ Detected\" if detected else \"✅ Not Detected\"\n",
    "                st.write(f\"{indicator.replace('_', ' ').title()}: {status}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
